"""
YolPedia Can Dede - AI Assistant for Alevi-Bektashi Philosophy
Refactored version with improved code quality, error handling, and maintainability
"""

import streamlit as st
import streamlit.components.v1 as components
import google.generativeai as genai
import json
import time
import random
import logging
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional, Generator
from pathlib import Path

# ===================== CONFIGURATION =====================

@dataclass
class AppConfig:
    """Application configuration constants"""
    MAX_MESSAGE_LIMIT: int = 30
    MIN_TIME_DELAY: int = 1
    RATE_LIMIT_WINDOW: int = 3600
    
    MIN_SEARCH_LENGTH: int = 3
    MAX_CONTENT_LENGTH: int = 1500
    
    # GÃœNCELLEME: BarajÄ± 30'a Ã§ektik. 
    # Sitenin kendi aramasÄ± gibi, iÃ§inde tek bir kelime geÃ§se bile yakalasÄ±n.
    SEARCH_SCORE_THRESHOLD: int = 30
    MAX_SEARCH_RESULTS: int = 5
    
    DATA_FILE: str = "yolpedia_data.json"
    ASSISTANT_NAME: str = "Can Dede | YolPedia Rehberiniz"
    MOTTO: str = '"BildiÄŸimin Ã¢limiyim, bilmediÄŸimin tÃ¢libiyim!"'
    
    YOLPEDIA_ICON: str = "https://yolpedia.eu/wp-content/uploads/2025/11/Yolpedia-favicon.png"
    CAN_DEDE_ICON: str = "https://yolpedia.eu/wp-content/uploads/2025/11/can-dede-logo.png"
    USER_ICON: str = "https://yolpedia.eu/wp-content/uploads/2025/11/group.png"
    
    GEMINI_MODELS: List[str] = None
    
    # Stop words listesi (Etkisiz kelimeler) - Normalize edilmiÅŸ halleri
    STOP_WORDS: List[str] = field(default_factory=lambda: [
        "ve", "veya", "ile", "bir", "bu", "su", "o", "icin", "hakkinda", 
        "kaynak", "kaynaklar", "ariyorum", "nedir", "kimdir", "nasil", 
        "ne", "var", "mi", "mu", "bana", "soyle", "goster", "ver", 
        "ilgili", "alakali", "yazi", "belge", "kitap", "makale", "soz", 
        "lutfen", "merhaba", "selam", "dedem", "can", "erenler", "konusunda", 
        "istiyorum", "elinde", "okur", "musun", "bul", "getir"
    ])
    
    def __post_init__(self):
        if self.GEMINI_MODELS is None:
            self.GEMINI_MODELS = [
                "gemini-1.5-flash",          
                "gemini-1.5-flash-latest",   
                "gemini-2.0-flash-exp",      
            ]

config = AppConfig()

# ===================== LOGGING SETUP =====================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===================== PAGE CONFIGURATION =====================

st.set_page_config(
    page_title=config.ASSISTANT_NAME,
    page_icon=config.YOLPEDIA_ICON,
    layout="centered"
)

# ===================== API KEY VALIDATION =====================

def get_api_keys() -> List[str]:
    """Retrieve and validate multiple API keys from secrets"""
    api_keys = []
    try:
        primary_key = st.secrets.get("API_KEY", "")
        if primary_key:
            api_keys.append(primary_key)
        
        secondary_key = st.secrets.get("API_KEY_2", "")
        if secondary_key:
            api_keys.append(secondary_key)
            
        third_key = st.secrets.get("API_KEY_3", "")
        if third_key:
            api_keys.append(third_key)
        
        if not api_keys:
            logger.error("No API keys found")
            return []
        
        logger.info(f"Loaded {len(api_keys)} API key(s)")
        return api_keys
        
    except Exception as e:
        logger.error(f"Failed to retrieve API keys: {e}")
        return []

GOOGLE_API_KEYS = get_api_keys()

if not GOOGLE_API_KEYS:
    st.error("âš ï¸ API anahtarÄ± bulunamadÄ±. LÃ¼tfen Streamlit secrets'Ä± kontrol edin.")
    st.stop()

# ===================== STYLING =====================

def apply_custom_styles():
    """Apply custom CSS styles"""
    st.markdown("""
    <style>
        .stChatMessage {
            margin-bottom: 10px;
        }
        .stSpinner > div {
            border-top-color: #ff4b4b !important;
        }
        .block-container {
            padding-top: 2rem;
        }
        h1 {
            line-height: 1.2 !important;
        }
    </style>
    """, unsafe_allow_html=True)

apply_custom_styles()

# ===================== DATA LOADING =====================

@st.cache_data(persist="disk", show_spinner=False)
def load_knowledge_base() -> List[Dict]:
    """Load knowledge base from JSON file with proper error handling"""
    try:
        file_path = Path(config.DATA_FILE)
        if not file_path.exists():
            logger.error(f"Data file not found: {config.DATA_FILE}")
            st.error(f"âŒ Veri dosyasÄ± bulunamadÄ±: {config.DATA_FILE}")
            return []
        
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            logger.info(f"Successfully loaded {len(data)} entries from knowledge base")
            return data
            
    except json.JSONDecodeError as e:
        logger.error(f"JSON decode error: {e}")
        st.error(f"âŒ JSON formatÄ± hatalÄ±: {e}")
        return []
    except Exception as e:
        logger.error(f"Unexpected error loading data: {e}")
        st.error(f"âŒ Veri yÃ¼klenirken beklenmeyen hata: {e}")
        return []

# ===================== TEXT PROCESSING =====================

def normalize_turkish_text(text: str) -> str:
    """
    Agresif Normalizasyon: TÃ¼rkÃ§e karakterleri, ÅŸapkalÄ± harfleri ve noktalÄ± harfleri
    standart Ä°ngilizce/ASCII karakterlerine Ã§evirir.
    """
    if not isinstance(text, str):
        return ""
    
    text = text.lower()
    
    # KapsamlÄ± deÄŸiÅŸim tablosu
    replacements = {
        # Standart TÃ¼rkÃ§e harfler
        "ÄŸ": "g", "Ä": "g",
        "Ã¼": "u", "Ãœ": "u",
        "ÅŸ": "s", "Å": "s",
        "Ä±": "i", "Ä°": "i",
        "Ã¶": "o", "Ã–": "o",
        "Ã§": "c", "Ã‡": "c",
        # ÅapkalÄ± (inceltme) harfler
        "Ã¢": "a", "Ã‚": "a",
        "Ã®": "i", "Ã": "i",
        "Ã»": "u", "Ã›": "u"
    }
    
    for src, dest in replacements.items():
        text = text.replace(src, dest)
        
    return text

# ===================== SESSION STATE INITIALIZATION =====================

def initialize_session_state():
    """Initialize all session state variables"""
    if 'db' not in st.session_state:
        st.session_state.db = load_knowledge_base()
    
    if "messages" not in st.session_state:
        st.session_state.messages = [{
            "role": "assistant",
            "content": (
                "Merhaba, Can Dost! Ben Can Dede. Sol menÃ¼den istediÄŸin modu seÃ§:\n\n"
                "â€¢ **Sohbet Modu:** Birlikte yol Ã¼zerine konuÅŸuruz, gÃ¶nÃ¼l muhabbeti ederiz.\n\n"
                "â€¢ **AraÅŸtÄ±rma Modu:** YolPedia arÅŸivinden sana kaynak sunarÄ±m.\n\n"
                "Buyur Erenler, hangi modda buluÅŸalÄ±m?"
            )
        }]
    
    if 'request_count' not in st.session_state:
        st.session_state.request_count = 0
    
    if 'last_reset_time' not in st.session_state:
        st.session_state.last_reset_time = time.time()
    
    if 'last_request_time' not in st.session_state:
        st.session_state.last_request_time = 0

initialize_session_state()

# ===================== RATE LIMITING =====================

def check_and_reset_rate_limit():
    """Check if rate limit window has expired and reset if needed"""
    current_time = time.time()
    if current_time - st.session_state.last_reset_time > config.RATE_LIMIT_WINDOW:
        st.session_state.request_count = 0
        st.session_state.last_reset_time = current_time
        logger.info("Rate limit counter reset")

def validate_rate_limit() -> Tuple[bool, str]:
    """Validate if user can make another request"""
    check_and_reset_rate_limit()
    
    if st.session_state.request_count >= config.MAX_MESSAGE_LIMIT:
        logger.warning(f"Rate limit exceeded: {st.session_state.request_count}")
        time_until_reset = int(config.RATE_LIMIT_WINDOW - (time.time() - st.session_state.last_reset_time))
        minutes = time_until_reset // 60
        return False, f"ğŸ›‘ Mesaj limitine ulaÅŸtÄ±nÄ±z ({config.MAX_MESSAGE_LIMIT} mesaj/saat). {minutes} dakika sonra tekrar deneyin."
    
    time_since_last = time.time() - st.session_state.last_request_time
    if time_since_last < config.MIN_TIME_DELAY:
        return False, "â³ LÃ¼tfen biraz yavaÅŸlayÄ±n, can..."
    
    return True, ""

# ===================== SEARCH ENGINE =====================

def calculate_relevance_score(entry: Dict, normalized_query: str, keywords: List[str]) -> int:
    """Calculate relevance score for a knowledge base entry"""
    score = 0
    
    normalized_title = normalize_turkish_text(entry.get('baslik', ''))
    normalized_content = normalize_turkish_text(entry.get('icerik', ''))
    
    # Tam eÅŸleÅŸme puanlarÄ±
    if normalized_query in normalized_title:
        score += 200
    elif normalized_query in normalized_content:
        score += 80 
    
    # Kelime bazlÄ± eÅŸleÅŸme
    for keyword in keywords:
        # BaÅŸlÄ±kta geÃ§en kelimeye Ã§ok yÃ¼ksek puan ver ki site aramasÄ± gibi Ã§alÄ±ÅŸsÄ±n
        if keyword in normalized_title:
            score += 100 
        elif keyword in normalized_content:
            score += 5 
    
    special_terms = ["gulbank", "deyis", "nefes", "siir"]
    if any(term in normalized_title for term in special_terms):
        score += 300
    
    return score

def search_knowledge_base(query: str, db: List[Dict]) -> Tuple[List[Dict], str]:
    """Search knowledge base for relevant content with STOP WORDS filtering"""
    if not db or not query or len(query) < config.MIN_SEARCH_LENGTH:
        return [], ""
    
    normalized_query = normalize_turkish_text(query)
    
    # Stop Words listesindeki kelimeleri Ã§Ä±kartÄ±yoruz
    keywords = [
        k for k in normalized_query.split() 
        if len(k) > 2 and k not in config.STOP_WORDS
    ]
    
    if not keywords:
        return [], normalized_query
        
    results = []
    for entry in db:
        score = calculate_relevance_score(entry, normalized_query, keywords)
        
        if score > config.SEARCH_SCORE_THRESHOLD:
            results.append({
                "baslik": entry.get('baslik', 'BaÅŸlÄ±ksÄ±z'),
                "link": entry.get('link', '#'),
                "icerik": entry.get('icerik', '')[:config.MAX_CONTENT_LENGTH],
                "puan": score
            })
    
    results.sort(key=lambda x: x['puan'], reverse=True)
    top_results = results[:config.MAX_SEARCH_RESULTS]
    
    logger.info(f"Search for '{query}' returned {len(top_results)} results. Keywords: {keywords}")
    return top_results, normalized_query

# ===================== LOCAL RESPONSE HANDLER =====================

def get_local_response(text: str) -> Optional[str]:
    """Check if query can be answered with predefined local responses"""
    normalized = normalize_turkish_text(text)
    
    greetings = ["merhaba", "selam", "selamun aleykum", "gunaydin"]
    status_queries = ["nasilsin", "naber", "ne var ne yok"]
    
    if any(g in normalized for g in greetings):
         return random.choice([
            "AÅŸk ile, merhaba gÃ¼zel can.",
            "Selam olsun. HoÅŸ geldin, sevgili dost.",
            "HoÅŸ geldin, can dost."
        ])
    
    if any(q in normalized for q in status_queries):
        return "ÅÃ¼kÃ¼r Hak'ka, yolun hizmetindeyiz erenler."
    
    return None

# ===================== AI RESPONSE GENERATOR =====================

def build_prompt(user_query: str, sources: List[Dict], mode: str) -> str:
    """Build the prompt for the AI model"""
    system_instruction = (
        "Sen 'Can Dede'sin. Alevi-BektaÅŸi felsefesini benimsemiÅŸ, insan-Ä± kÃ¢mil bir rehbersin. "
        "Ãœslubun 'AÅŸk ile', 'Can', 'Erenler' ÅŸeklinde samimi ve sÄ±cak olsun."
    )
    
    if "Sohbet" in mode:
        if sources:
            source_text = "\n".join([
                f"- {src['baslik']}: {src['icerik']}"
                for src in sources[:2]
            ])
            return (
                f"{system_instruction}\n\n"
                f"KAYNAKLAR (BunlarÄ± kullanarak cevapla):\n{source_text}\n\n"
                f"KullanÄ±cÄ±: {user_query}"
            )
        else:
            return f"{system_instruction}\n\nKullanÄ±cÄ±: {user_query}"
            
    else:  # Research mode
        if not sources:
            return None
        
        source_text = "\n".join([
            f"- {src['baslik']}: {src['icerik'][:800]}"
            for src in sources[:3]
        ])
        return (
            f"Sen YolPedia asistanÄ±sÄ±n. GÃ¶revin sadece aÅŸaÄŸÄ±daki KAYNAKLARI kullanarak cevap vermektir.\n"
            f"EÄŸer sorunun cevabÄ± kaynaklarda yoksa, KESÄ°NLÄ°KLE uydurma ve 'ArÅŸivde bu konuda bilgi bulamadÄ±m' de.\n"
            f"Asla kaynaklarÄ±n dÄ±ÅŸÄ±na Ã§Ä±kma.\n\n"
            f"KAYNAKLAR:\n{source_text}\n\n"
            f"Soru: {user_query}"
        )

def generate_ai_response(
    user_query: str,
    sources: List[Dict],
    mode: str
) -> Generator[str, None, None]:
    """
    Generate AI response using Google Gemini API with VISIBLE robust key rotation.
    """
    
    # 1. Ã–nce yerel veritabanÄ±na bak
    local_response = get_local_response(user_query)
    if local_response:
        time.sleep(0.5)
        yield local_response
        return
    
    # 2. ARAÅTIRMA MODU KORUMASI
    if "AraÅŸtÄ±rma" in mode and not sources:
        yield "ğŸ“š ÃœzgÃ¼nÃ¼m can, YolPedia arÅŸivinde bu konuyla ilgili yeterli kaynak bulunamadÄ±. BaÅŸka bir konuda yardÄ±mcÄ± olabilir miyim?"
        return

    # 3. Prompt hazÄ±rla
    prompt = build_prompt(user_query, sources, mode)
    if prompt is None:
        yield "ğŸ“š AradÄ±ÄŸÄ±n konuyla ilgili kaynak bulamadÄ±m can."
        return
    
    success = False
    last_error_details = ""
    status_box = st.empty()

    for key_index, current_api_key in enumerate(GOOGLE_API_KEYS):
        
        if success:
            break
            
        try:
            genai.configure(api_key=current_api_key)
            
            for model_name in config.GEMINI_MODELS:
                try:
                    model = genai.GenerativeModel(model_name)
                    generation_config = {
                        "temperature": 0.3,
                        "max_output_tokens": 1500,
                    }
                    
                    response = model.generate_content(
                        prompt, 
                        stream=True,
                        generation_config=generation_config
                    )
                    
                    has_content = False
                    for chunk in response:
                        if chunk.text:
                            status_box.empty()
                            yield chunk.text
                            has_content = True
                    
                    if has_content:
                        success = True
                        break 
                        
                except Exception as model_error:
                    error_msg = str(model_error).lower()
                    
                    if "429" in error_msg or "quota" in error_msg or "exhausted" in error_msg:
                        time.sleep(1)
                        last_error_details = f"Anahtar {key_index+1} KotasÄ± Dolu (429)"
                        break 
                    
                    logger.warning(f"Model hatasÄ±: {model_name} -> {model_error}")
                    continue

        except Exception as key_error:
            last_error_details = str(key_error)
            continue
            
    if not success:
        status_box.error("âŒ TÃ¼m denemeler baÅŸarÄ±sÄ±z oldu.")
        yield f"âš ï¸ Can dost, elimdeki {len(GOOGLE_API_KEYS)} farklÄ± anahtarÄ±n hepsini denedim ama Google kapÄ±larÄ± kapalÄ± tutuyor. \n\n**Son Hata DetayÄ±:** {last_error_details}\n\nLÃ¼tfen 2-3 dakika bekleyip tekrar dene."
    else:
        status_box.empty()

# ===================== UI HELPER FUNCTIONS =====================

def scroll_to_bottom():
    """Scroll chat to bottom using JavaScript"""
    components.html(
        """
        <script>
            window.parent.document.querySelector(".main").scrollTop = 100000;
        </script>
        """,
        height=0
    )

def render_header():
    """Render application header"""
    st.markdown(f"""
    <div style="text-align: center; margin-bottom: 30px;">
        <div style="display: flex; justify-content: center; margin-bottom: 20px;">
            <img src="{config.YOLPEDIA_ICON}" style="width: 60px; height: auto;">
        </div>
        <div style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 10px;">
            <img src="{config.CAN_DEDE_ICON}" 
                 style="width: 50px; height: 50px; border-radius: 50%; object-fit: cover; border: 2px solid #eee;">
            <h1 style="margin: 0; font-size: 34px; font-weight: 700; color: #ffffff;">
                {config.ASSISTANT_NAME}
            </h1>
        </div>
        <div style="font-size: 16px; font-style: italic; color: #cccccc; font-family: 'Georgia', serif;">
            {config.MOTTO}
        </div>
    </div>
    """, unsafe_allow_html=True)

def render_sidebar() -> str:
    """Render sidebar and return selected mode"""
    with st.sidebar:
        st.title("Mod SeÃ§imi")
        selected_mode = st.radio(
            "Can Dede nasÄ±l yardÄ±mcÄ± olsun?",
            ["Sohbet Modu", "AraÅŸtÄ±rma Modu"]
        )
        
        if st.button("ğŸ—‘ï¸ Sohbeti SÄ±fÄ±rla"):
            st.session_state.messages = [{
                "role": "assistant",
                "content": "Sohbet sÄ±fÄ±rlandÄ±. Buyur can. Sendeyim yine."
            }]
            st.session_state.request_count = 0
            logger.info("Chat history reset by user")
            st.rerun()
        
        st.divider()
        st.caption(f"ğŸ“Š Mesaj: {st.session_state.request_count}/{config.MAX_MESSAGE_LIMIT}")
        
        # GÃœNCELLEME: Ä°STEDÄ°ÄÄ°NÄ°Z Ã–ZELLÄ°K EKLENDÄ°
        # Toplam kaynak sayÄ±sÄ±nÄ± veritabanÄ±ndan Ã§ekip gÃ¶sterir
        if 'db' in st.session_state:
            total_sources = len(st.session_state.db)
            st.info(f"ğŸ“š ArÅŸivdeki Toplam Kaynak: **{total_sources}**")
        else:
             st.warning("âš ï¸ VeritabanÄ± yÃ¼klenemedi!")
        
    return selected_mode

def render_sources(sources: List[Dict]):
    """Render source references"""
    if not sources:
        return
    
    st.markdown("---")
    st.markdown("**ğŸ“š Kaynaklar:**")
    for source in sources[:3]:
        st.markdown(f"â€¢ [{source['baslik']}]({source['link']})")

# ===================== MAIN APPLICATION =====================

def main():
    """Main application flow"""
    # Render UI components
    render_header()
    selected_mode = render_sidebar()
    
    # Display chat history
    for message in st.session_state.messages:
        icon = config.CAN_DEDE_ICON if message["role"] == "assistant" else config.USER_ICON
        with st.chat_message(message["role"], avatar=icon):
            st.markdown(message["content"])
    
    # Handle user input
    user_input = st.chat_input("Can Dede'ye sor...")
    
    if user_input:
        can_proceed, error_message = validate_rate_limit()
        if not can_proceed:
            st.error(error_message)
            st.stop()
        
        st.session_state.request_count += 1
        st.session_state.last_request_time = time.time()
        
        st.session_state.messages.append({"role": "user", "content": user_input})
        st.chat_message("user", avatar=config.USER_ICON).markdown(user_input)
        scroll_to_bottom()
        
        sources, _ = search_knowledge_base(user_input, st.session_state.db)
        
        with st.chat_message("assistant", avatar=config.CAN_DEDE_ICON):
            placeholder = st.empty()
            full_response = ""
            
            with st.spinner("Can Dede tefekkÃ¼rde..."):
                for chunk in generate_ai_response(user_input, sources, selected_mode):
                    full_response += chunk
                    placeholder.markdown(full_response + "â–Œ")
            
            placeholder.markdown(full_response)
            
            # GÃœNCELLEME: Can Dede "BulamadÄ±m" derse kaynaklarÄ± gizle!
            failure_phrases = ["bilgi bulamadÄ±m", "kaynak bulamadÄ±m", "yeterli kaynak", "Ã¼zgÃ¼nÃ¼m"]
            is_failure = any(phrase in full_response.lower() for phrase in failure_phrases)
            
            if sources and "AraÅŸtÄ±rma" in selected_mode and not is_failure:
                render_sources(sources)
            
            st.session_state.messages.append({
                "role": "assistant",
                "content": full_response
            })
            scroll_to_bottom()

if __name__ == "__main__":
    main()
